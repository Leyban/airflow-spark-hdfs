{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import Row\n",
    "from datetime import date\n",
    "from pyspark.sql.functions import lit\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/10 15:54:41 WARN Utils: Your hostname, tns-des180 resolves to a loopback address: 127.0.1.1; using 172.24.31.35 instead (on interface enp2s0)\n",
      "23/08/10 15:54:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/10 15:54:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Fetch_PGSoft_to_HDFS\") \\\n",
    "    .master(\"local\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PGSOFT_OLD_VERSION_TABLE ='pgsoft_old_version'\n",
    "\n",
    "secret_key = \"\"\n",
    "operator_token = \"\"\n",
    "pg_history_url = \"\"\n",
    "\n",
    "history_api = '/v2/Bet/GetHistory'\n",
    "\n",
    "# url = f\"{pg_history_url}{history_api}\" \n",
    "\n",
    "url = \"http://localhost:8800/pg_soft\" # MockAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This can be executed in AIRFLOW then pass it via xcom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
    "\n",
    "def get_pgversion():\n",
    "    conn_collector_pg_hook = PostgresHook(postgres_conn_id='collector_conn_id')\n",
    "    query = \"\"\"\n",
    "        SELECT row_version FROM {0} LIMIT 1\n",
    "    \"\"\".format(PGSOFT_OLD_VERSION_TABLE)\n",
    "\n",
    "    df = conn_collector_pg_hook.get_pandas_df(query)\n",
    "    if not df.empty:\n",
    "        latest_row_version = df['row_version'].iloc[0]\n",
    "        return latest_row_version\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_some_value(**kwargs):\n",
    "#     some_value = 10\n",
    "#     return some_value\n",
    "\n",
    "# task1 = PythonOperator(task_id='run_task_1',\n",
    "#                        python_callable=get_some_value,\n",
    "#                        provide_context=True,\n",
    "#                        dag=dag)\n",
    "\n",
    "# task2 = SparkSubmitOperator(\n",
    "#     task_id='run_sparkSubmit_job',\n",
    "#     conn_id='spark_default',\n",
    "#     java_class='com.example',\n",
    "#     application='example.jar',\n",
    "#     name='airflow-spark-job',\n",
    "#     verbose=True,\n",
    "#     application_args=[\"{{ti.xcom_pull(task_ids='get_pgsoft_row_version')}}\"],  \n",
    "#     conf={'master':'yarn'},\n",
    "#     dag=dag,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab the row version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "latest_row_version = sys.argv[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start download pg: row_version --ip=127.0.0.1\n",
      "response 5000\n"
     ]
    }
   ],
   "source": [
    "# try:\n",
    "form_data = {\n",
    "    \"secret_key\":     secret_key,\n",
    "    \"operator_token\": operator_token,\n",
    "    \"bet_type\":        \"1\",\n",
    "    \"row_version\":  latest_row_version,\n",
    "    \"count\":          \"5000\"\n",
    "}\n",
    "\n",
    "print(f\"Start download pg: row_version {latest_row_version}\")\n",
    "response = requests.post(url, data=form_data)\n",
    "response.raise_for_status() \n",
    "print(f\"response {len(response.json())}\")\n",
    "\n",
    "json_data = response.json()\n",
    "df = spark.createDataFrame(json_data) \n",
    "\n",
    "# options = { 'url' : url, 'method' : 'GET', 'readTimeout' : '10000', 'connectionTimeout' : '2000', 'partitions' : '10'}\n",
    "\n",
    "# df = spark.read.format(\"org.apache.dsext.spark.datasource.rest.RestDataSource\").options(**options).load()\n",
    "\n",
    "# if response.status_code == 404:\n",
    "#     print(\"Error 404: Not Found\")\n",
    "# else:\n",
    "#     json_content = response.json()\n",
    "#     print(json_content)\n",
    "\n",
    "# except requests.exceptions.RequestException as err:\n",
    "# print(\"Request error:\", err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- balanceAfter: long (nullable = true)\n",
      " |-- balanceBefore: long (nullable = true)\n",
      " |-- betAmount: long (nullable = true)\n",
      " |-- betId: long (nullable = true)\n",
      " |-- betTime: string (nullable = true)\n",
      " |-- betType: long (nullable = true)\n",
      " |-- create_at: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- gameId: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- jackpotRtpContributionAmount: long (nullable = true)\n",
      " |-- jackpotWinAmount: long (nullable = true)\n",
      " |-- parentBetId: long (nullable = true)\n",
      " |-- platform: long (nullable = true)\n",
      " |-- playerName: string (nullable = true)\n",
      " |-- rowVersion: long (nullable = true)\n",
      " |-- transactionType: long (nullable = true)\n",
      " |-- update_at: string (nullable = true)\n",
      " |-- winAmount: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, year, quarter, date_format, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = df \\\n",
    "    .withColumn(\"betTime\",to_timestamp(df[\"betTime\"])) \\\n",
    "    .withColumn(\"year\", date_format(col(\"betTime\"), \"yyyy\")) \\\n",
    "    .withColumn(\"quarter\", date_format(col(\"betTime\"), \"Q\")) \\\n",
    "    .withColumn(\"year_quarter\", date_format(col(\"betTime\"), \"'p'yyyy'q'Q\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+---------+------+-------------------+-------+--------------------+--------+------+---+----------------------------+----------------+-----------+--------+----------+----------+---------------+--------------------+---------+----+-------+------------+\n",
      "|balanceAfter|balanceBefore|betAmount| betId|            betTime|betType|           create_at|currency|gameId| id|jackpotRtpContributionAmount|jackpotWinAmount|parentBetId|platform|playerName|rowVersion|transactionType|           update_at|winAmount|year|quarter|year_quarter|\n",
      "+------------+-------------+---------+------+-------------------+-------+--------------------+--------+------+---+----------------------------+----------------+-----------+--------+----------+----------+---------------+--------------------+---------+----+-------+------------+\n",
      "|         177|          548|      292|345540|2000-06-22 07:26:46|    997|2018-03-24T15:24:06Z|   nykoq|   687|  1|                         938|             483|     977198|     334|     vrpzl|       349|            913|1985-05-20T22:03:35Z|      153|2000|      2|     p2000q2|\n",
      "|         683|          543|      566|314826|1989-07-18 14:26:49|    244|1974-05-22T18:19:35Z|   muctm|    41|  2|                         794|             289|     975350|     643|     ltwor|       997|              4|2005-10-26T06:27:58Z|      118|1989|      3|     p1989q3|\n",
      "|         669|           94|      198|271690|2016-06-15 14:09:52|   1020|1999-02-16T16:45:17Z|   nngll|   136|  3|                         362|             218|     260270|     539|     zcflv|       504|            129|2019-04-01T06:45:32Z|       77|2016|      2|     p2016q2|\n",
      "|         985|           53|      120|565542|1983-04-18 10:54:04|    163|1987-04-26T10:56:36Z|   uqtch|   440|  4|                          38|              51|     345547|     236|     fhuuu|       285|             35|2008-01-05T14:54:45Z|      265|1983|      2|     p1983q2|\n",
      "|         298|          385|      588|996140|2018-05-26 18:12:07|    336|2023-07-05T23:03:33Z|   tfkgu|   130|  5|                         630|             539|     456344|     791|     nvoky|       994|            240|1983-10-17T13:12:10Z|      668|2018|      2|     p2018q2|\n",
      "|         815|          957|       44|597086|2003-10-22 17:10:49|    641|1978-06-30T15:32:32Z|   ihbwo|   866|  6|                         531|              20|     786133|     625|     iowzz|      1015|            833|1978-01-08T09:13:08Z|       20|2003|      4|     p2003q4|\n",
      "|         491|          689|      839|746246|1970-12-04 21:34:03|    896|1996-08-05T19:56:21Z|   ufesu|    51|  7|                         567|             959|     790340|     530|     aptkk|       802|            765|1971-01-27T11:09:10Z|      257|1970|      4|     p1970q4|\n",
      "|         462|          583|      213| 59700|1996-11-08 16:52:43|    256|2019-11-26T01:52:11Z|   trflt|    93|  8|                         567|             899|     417684|     523|     nehns|       980|            565|1985-10-15T17:53:50Z|      767|1996|      4|     p1996q4|\n",
      "|         274|         1005|      647|670887|1992-01-04 11:18:14|    225|1982-04-03T02:28:19Z|   tbynb|   438|  9|                         111|             708|     180669|     925|     irwpq|       577|            477|1992-08-01T05:24:55Z|      475|1992|      1|     p1992q1|\n",
      "|         588|          436|       52|128994|1989-04-10 01:57:47|    553|1980-12-14T13:51:39Z|   mcrxj|   765| 10|                         864|             566|     341941|      31|     snmbz|       263|            608|2014-11-24T22:17:39Z|      596|1989|      2|     p1989q2|\n",
      "|         873|          231|      667|117840|1999-07-30 03:06:54|    554|2022-08-05T10:28:34Z|   emyfq|   894| 11|                         205|             468|     471883|     115|     uzidl|       240|            972|1998-12-30T05:39:38Z|      231|1999|      3|     p1999q3|\n",
      "|         326|           41|      604|300108|2020-03-15 06:12:32|    246|1977-11-18T12:22:30Z|   ipjev|   220| 12|                         560|             675|     789456|      31|     wjfuf|       310|            770|2010-03-09T22:07:13Z|      870|2020|      1|     p2020q1|\n",
      "|         331|          307|      694|893315|1971-12-16 23:47:37|    697|2022-05-07T12:35:36Z|   afoed|   302| 13|                         310|             432|     308350|     658|     awcft|       444|            680|2010-12-12T17:43:46Z|      732|1971|      4|     p1971q4|\n",
      "|         833|          515|      948|174713|1972-12-28 14:41:59|    124|2002-01-09T17:07:24Z|   aqojc|   490| 14|                         892|             948|     606339|     595|     cwnmn|       822|            461|1988-01-14T09:10:18Z|      701|1972|      4|     p1972q4|\n",
      "|          39|          486|       41|824981|1973-01-05 08:12:02|    179|1983-12-19T02:28:44Z|   yiafn|   479| 15|                         359|             823|      29086|     351|     fcrdi|       755|            939|2015-07-08T22:16:35Z|      670|1973|      1|     p1973q1|\n",
      "|         373|          789|      590| 27178|1991-11-23 04:44:27|    250|1972-07-05T20:33:32Z|   zkorb|   847| 16|                         193|             428|     332230|     633|     hwccw|        13|            463|1982-01-06T09:30:44Z|      267|1991|      4|     p1991q4|\n",
      "|         728|          977|      504|607748|2000-12-05 14:04:30|    704|2007-04-05T10:28:05Z|   lbhus|   841| 17|                         633|              48|     270128|     431|     mmfvi|       906|            418|1998-01-17T11:21:05Z|      327|2000|      4|     p2000q4|\n",
      "|         508|          812|      579|492776|1980-10-26 23:41:29|    410|1975-09-26T07:53:29Z|   qwcwp|   106| 18|                         981|               3|     942748|     905|     gtahw|        58|            700|2014-11-15T07:48:31Z|      619|1980|      4|     p1980q4|\n",
      "|         775|          760|      120|415410|1994-08-22 20:45:32|    445|1990-05-20T04:48:28Z|   xscdh|   151| 19|                         513|             776|     288610|     396|     xwnvq|       793|             99|2001-04-11T03:20:31Z|      586|1994|      3|     p1994q3|\n",
      "|         940|          444|      554|513931|2008-10-31 15:11:06|    778|2008-07-19T06:14:45Z|   ihxob|   187| 20|                         299|             268|     328749|     303|     cksik|       472|            511|2011-05-04T01:17:54Z|      163|2008|      4|     p2008q4|\n",
      "+------------+-------------+---------+------+-------------------+-------+--------------------+--------+------+---+----------------------------+----------------+-----------+--------+----------+----------+---------------+--------------------+---------+----+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- balanceAfter: long (nullable = true)\n",
      " |-- balanceBefore: long (nullable = true)\n",
      " |-- betAmount: long (nullable = true)\n",
      " |-- betId: long (nullable = true)\n",
      " |-- betTime: timestamp (nullable = true)\n",
      " |-- betType: long (nullable = true)\n",
      " |-- create_at: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- gameId: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- jackpotRtpContributionAmount: long (nullable = true)\n",
      " |-- jackpotWinAmount: long (nullable = true)\n",
      " |-- parentBetId: long (nullable = true)\n",
      " |-- platform: long (nullable = true)\n",
      " |-- playerName: string (nullable = true)\n",
      " |-- rowVersion: long (nullable = true)\n",
      " |-- transactionType: long (nullable = true)\n",
      " |-- update_at: string (nullable = true)\n",
      " |-- winAmount: long (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- quarter: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tblLocation = './user/hive/datalake/wagers/pgsoft'\n",
    "out_df.write.partitionBy('year', 'quarter').mode('append').parquet(tblLocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tblLocation = './user/hive/datalake/wagers/pgsoft2'\n",
    "out_df.write.partitionBy('year_quarter').mode('append').parquet(tblLocation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
